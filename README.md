
# ğŸ›¡ï¸ LLM Security: DetecÃ§Ã£o e MitigaÃ§Ã£o de Prompt Injection Attacks com RLHF

Este repositÃ³rio contÃ©m o cÃ³digo-fonte e os scripts utilizados no experimento de fine-tuning supervisionado com feedback humano para mitigar ataques de Prompt Injection em modelos de linguagem (LLMs), utilizando o modelo base GPT-2 e a biblioteca HuggingFace Transformers.

## ğŸ“š DescriÃ§Ã£o do Projeto

Prompt Injection Attacks consistem em manipular o comportamento de modelos de linguagem atravÃ©s de comandos maliciosos inseridos no prompt. Este projeto explora uma abordagem bioinspirada de aprendizado por reforÃ§o supervisionado humano para identificar e mitigar essas injeÃ§Ãµes, simulando um processo de curadoria e refinamento iterativo.

O pipeline Ã© dividido em trÃªs etapas principais:

1. **GeraÃ§Ã£o de Respostas**: respostas sÃ£o geradas automaticamente para prompts contendo ou nÃ£o ataques.
2. **Fine-Tuning com Feedback Humano**: um dataset anotado com scores humanos Ã© usado para refinar o modelo.
3. **AvaliaÃ§Ã£o Comparativa**: sÃ£o comparadas respostas do modelo original vs modelo refinado.

## ğŸ› ï¸ Estrutura do RepositÃ³rio

````
ğŸ“ llm-security-rlhf/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.csv               # Prompts originais (com/sem injeÃ§Ãµes)
â”‚   â”œâ”€â”€ generated\_responses.csv   # Respostas do modelo base
â”‚   â”œâ”€â”€ evaluated\_responses.csv   # Respostas anotadas com score
â”‚   â””â”€â”€ model\_comparison.csv      # Comparativo final entre modelos
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate\_responses.py     # Gera respostas com GPT-2
â”‚   â”œâ”€â”€ finetune\_model.py         # Fine-tuning supervisionado com score > 1
â”‚   â””â”€â”€ evaluate\_model.py         # Compara desempenho dos modelos
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

````

## ğŸš€ Como Rodar o Projeto

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/llm-security-rlhf.git
cd llm-security-rlhf
````

### 2. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

> Requisitos principais: `transformers`, `datasets`, `torch`, `pandas`, `scikit-learn`, `tqdm`

### 3. Execute os scripts

#### Gerar respostas

```bash
python scripts/generate_responses.py
```

#### Aplicar fine-tuning supervisionado

> Certifique-se de anotar o arquivo `generated_responses.csv` com colunas `score`, `prompt_traduzido`, `resposta`.

```bash
python scripts/finetune_model.py
```

#### Avaliar modelos (original vs. fine-tuned)

```bash
python scripts/evaluate_model.py
```

---

## ğŸ“ˆ Resultados Esperados

* Melhoria na qualidade e seguranÃ§a das respostas do modelo refinado
* ReduÃ§Ã£o da suscetibilidade a prompts maliciosos
* Dataset com anotaÃ§Ãµes humanas Ãºteis para estudos futuros sobre alinhamento de LLMs

---

## ğŸ’¡ Tecnologias Utilizadas

* [HuggingFace Transformers](https://huggingface.co/transformers/)
* PyTorch
* Pandas
* Python 3.10+
* CUDA (opcional para aceleraÃ§Ã£o com GPU)

---

## ğŸ§  Autor

Desenvolvido por \[Ana Caroline Silva Pontara] â€“ Mestranda em CiÃªncia da ComputaÃ§Ã£o na UNESP, membro do [LARS - Advanced Network and Security Lab](https://www.keltoncosta.com.br/LARS/index.html)

---

## ğŸ“œ LicenÃ§a

Este projeto Ã© livre para uso acadÃªmico. Para fins comerciais ou redistribuiÃ§Ã£o, entre em contato com os autores.
